{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T14:23:04.812235Z",
     "start_time": "2019-12-26T14:23:04.541249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "\n",
      "1.13.1+cu117\n",
      "\n",
      "document reference : https://www.i32n.com/docs/pytorch/tutorials/beginner/basics/intro.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as th \n",
    "from torch import nn\n",
    "np.random.seed(seed=42)\n",
    "print(\n",
    "    np.__version__,\n",
    "    th.__version__,\n",
    "    'document reference : https://www.i32n.com/docs/pytorch/tutorials/beginner/basics/intro.html',\n",
    "    sep='\\n\\n'\n",
    ")\n",
    "\n",
    "\n",
    "# Reference\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/basics/intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model with nn.Nodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if th.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if th.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EGES(\n",
       "  (side_info_weights): Embedding(400, 2)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cp_spark_ml.algo.eges.model import EGES\n",
    "\n",
    "EGES(\n",
    "    dim=16, num_nodes=400, side_info_name_with_n_unique_values={'n_shop':2}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.0247, 0.6037],\n",
      "        [0.9539, 0.1645]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = th.tensor(data)\n",
    "\n",
    "print(x_data)\n",
    "\n",
    "np_array = np.array(data)\n",
    "x_np = th.from_numpy(np_array)\n",
    "\n",
    "print(x_np)\n",
    "\n",
    "x_ones = th.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = th.rand_like(x_data, dtype=th.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.9303, 0.8023, 0.2242],\n",
      "        [0.7083, 0.8109, 0.1115]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = th.rand(shape)\n",
    "ones_tensor = th.ones(shape)\n",
    "zeros_tensor = th.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = th.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "is_gpu = th.cuda.is_available()\n",
    "print(is_gpu)\n",
    "if is_gpu:\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = th.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Concat tensor\n",
    "t1 = th.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = th.rand_like(y1)\n",
    "th.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = th.rand_like(tensor)\n",
    "th.mul(tensor, tensor, out=z3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Th <--> Np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = th.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = th.from_numpy(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReShaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) torch.Size([1, 3]) torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.Tensor.view.html\n",
    "\n",
    "t = th.tensor([1,2,3])\n",
    "t2 = t.unsqueeze(0)\n",
    "t3 = t.unsqueeze(0)\n",
    "# t4 = t.view([0])\n",
    "print(t.shape, t2.shape, t3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIN (Deep Interest Network)\n",
    "\n",
    "* https://github.com/fanoping/DIN-pytorch/tree/master/din\n",
    "* http://arxiv.org/pdf/1706.06978"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice Activation Function\n",
    "\n",
    "* Basically, regulaization based on the id frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class Dice(nn.Module):\n",
    "    def __init__(self, num_features, dim=2):\n",
    "        super(Dice, self).__init__()\n",
    "        assert dim == 2 or dim == 3\n",
    "        self.bn = nn.BatchNorm1d(num_features, eps=1e-9)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dim = dim\n",
    "        \n",
    "        if self.dim == 3:\n",
    "            # self.alpha = th.zeros((num_features, 1)).cuda()\n",
    "            self.alpha = th.zeros((num_features, 1))\n",
    "        elif self.dim == 2:\n",
    "            # self.alpha = th.zeros((num_features,)).cuda()\n",
    "            self.alpha = th.zeros((num_features,))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.dim == 3:\n",
    "            x = th.transpose(x, 1, 2)\n",
    "            x_p = self.sigmoid(self.bn(x))\n",
    "            out = self.alpha * (1 - x_p) * x + x_p * x\n",
    "            out = th.transpose(out, 1, 2)\n",
    "        \n",
    "        elif self.dim == 2:\n",
    "            x_p = self.sigmoid(self.bn(x))\n",
    "            out = self.alpha * (1 - x_p) * x + x_p * x\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32])\n"
     ]
    }
   ],
   "source": [
    "a = Dice(32)\n",
    "b = torch.zeros((10, 32))\n",
    "    #b = torch.transpose(b, 1, 2)\n",
    "c = a(b)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# from .dice import Dice\n",
    "#from dice import Dice\n",
    "\n",
    "class FullyConnectedLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias, batch_norm=True, dropout_rate=0.5, activation='relu', sigmoid=False, dice_dim=2):\n",
    "        super(FullyConnectedLayer, self).__init__()\n",
    "        assert len(hidden_size) >= 1 and len(bias) >= 1\n",
    "        assert len(bias) == len(hidden_size)\n",
    "        self.sigmoid = sigmoid\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_size[0], bias=bias[0]))\n",
    "        \n",
    "        for i, h in enumerate(hidden_size[:-1]):\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size[i]))\n",
    "            \n",
    "            if activation.lower() == 'relu':\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "            elif activation.lower() == 'dice':\n",
    "                assert dice_dim\n",
    "                layers.append(Dice(hidden_size[i], dim=dice_dim))\n",
    "            elif activation.lower() == 'prelu':\n",
    "                layers.append(nn.PReLU())\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            layers.append(nn.Dropout(p=dropout_rate))\n",
    "            layers.append(nn.Linear(hidden_size[i], hidden_size[i+1], bias=bias[i]))\n",
    "        \n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        if self.sigmoid:\n",
    "            self.output_layer = nn.Sigmoid()\n",
    "        \n",
    "        # weight initialization xavier_normal (or glorot_normal in keras, tf)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=1.0)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output_layer(self.fc(x)) if self.sigmoid else self.fc(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]) FullyConnectedLayer(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=200, bias=True)\n",
      "    (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=200, out_features=80, bias=True)\n",
      "    (5): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=80, out_features=1, bias=True)\n",
      "  )\n",
      ") tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddmmBackward0>) torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "a = FullyConnectedLayer(input_size=2, hidden_size=[200, 80, 1], bias=[True,True,True])\n",
    "b = torch.zeros((3, 2))\n",
    "out = a(b)\n",
    "print(\n",
    "    b,\n",
    "    a,\n",
    "    out,\n",
    "    out.size()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LocalAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalActivationUnit(nn.Module):\n",
    "    def __init__(self, hidden_size=[80, 40], bias=[True, True], embedding_dim=4, batch_norm=False):\n",
    "        super(LocalActivationUnit, self).__init__()\n",
    "        self.fc1 = FullyConnectedLayer(input_size=4*embedding_dim,\n",
    "                                       hidden_size=hidden_size,\n",
    "                                       bias=bias,\n",
    "                                       batch_norm=batch_norm,\n",
    "                                       activation='dice',\n",
    "                                       dice_dim=3)\n",
    "\n",
    "        self.fc2 = FullyConnectedLayer(input_size=hidden_size[-1],\n",
    "                                       hidden_size=[1],\n",
    "                                       bias=[True],\n",
    "                                       batch_norm=batch_norm,\n",
    "                                       activation='dice',\n",
    "                                       dice_dim=3)\n",
    "        # TODO: fc_2 initialization\n",
    "\n",
    "    def forward(self, query, user_behavior):\n",
    "        # query ad            : size -> batch_size * 1 * embedding_size\n",
    "        # user behavior       : size -> batch_size * time_seq_len * embedding_size\n",
    "\n",
    "        user_behavior_len = user_behavior.size(1)\n",
    "        queries = torch.cat([query for _ in range(user_behavior_len)], dim=1)\n",
    "\n",
    "        attention_input = torch.cat([queries, user_behavior, queries-user_behavior, queries*user_behavior], dim=-1)\n",
    "        attention_output = self.fc1(attention_input)\n",
    "        attention_output = self.fc2(attention_output)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSequencePoolingLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim=4):\n",
    "        super(AttentionSequencePoolingLayer, self).__init__()\n",
    "\n",
    "        # TODO: DICE acitivation function\n",
    "        # TODO: attention weight normalization\n",
    "        self.local_att = LocalActivationUnit(hidden_size=[64, 16], bias=[True, True], embedding_dim=embedding_dim, batch_norm=False)\n",
    "\n",
    "    \n",
    "    def forward(self, query_ad, user_behavior, user_behavior_length):\n",
    "        # query ad            : size -> batch_size * 1 * embedding_size\n",
    "        # user behavior       : size -> batch_size * time_seq_len * embedding_size\n",
    "        # user behavior length: size -> batch_size * 1\n",
    "        # output              : size -> batch_size * 1 * embedding_size\n",
    "        \n",
    "        attention_score = self.local_att(query_ad, user_behavior)\n",
    "        attention_score = th.transpose(attention_score, 1, 2)  # B * 1 * T\n",
    "        #print(attention_score.size())\n",
    "        \n",
    "        # define mask by length\n",
    "        user_behavior_length = user_behavior_length.type(th.LongTensor)\n",
    "        mask = th.arange(user_behavior.size(1))[None, :] < user_behavior_length[:, None]\n",
    "        \n",
    "        # mask\n",
    "        output = th.mul(attention_score, mask.type(th.FloatTensor))  # batch_size *\n",
    "\n",
    "        # multiply weight\n",
    "        output = th.matmul(output, user_behavior)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = AttentionSequencePoolingLayer()\n",
    "b = th.zeros((3, 1, 4))\n",
    "c = th.zeros((3, 20, 4))\n",
    "d = th.ones((3, 1))\n",
    "a(b, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "cp_spark_ml",
   "language": "python",
   "name": "cp_spark_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
